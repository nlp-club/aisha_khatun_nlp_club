{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "fastai nlp 3b-more-details.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "LLMbqynqcluI",
        "colab_type": "text"
      },
      "source": [
        "## Trouble installing fastai library?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "NTeGE8ALcluL",
        "colab_type": "text"
      },
      "source": [
        "Here is a [guide to troubleshooting](https://docs.fast.ai/troubleshoot.html) problems with fastai installation.  By far, the most common problem is having fastai installed for a different environment/different Python installation than the one your Jupyter notebook is using (you can have Python installed in multiple places on your computer and not even realize it!). Or, you might have different versions of fastai installed in your different environments/different Python installations (and the one you are running in Jupyter notebook could be out of date, even if you installed version 1.0 somewhere else). For both of these problems, please [see this entry](https://docs.fast.ai/troubleshoot.html#modulenotfounderror-no-module-named-fastaivision)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "q4NWga4QcluN",
        "colab_type": "text"
      },
      "source": [
        "## More detail about randomized SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "EWsboKbXcluP",
        "colab_type": "text"
      },
      "source": [
        "I didn't cover how randomized SVD worked, because we aren't going to learn about it in detail in this course.  The main things I want you to know about randomized SVD are:\n",
        "- **it is fast**\n",
        "- **it gives us a truncated SVD** (whereas with traditional SVD, we are usually throwing away small singular values and their corresponding columns)\n",
        "\n",
        "If you were curious to know more, two keys are:\n",
        "- It is often useful to be able to reduce dimensionality of data in a way that preserves distances. The Johnsonâ€“Lindenstrauss lemma is a classic result of this type.  [Johnson-Lindenstrauss Lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma): a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved (proof uses random projections).\n",
        "- We haven't found a better general SVD method, we'll just use the method we have on a smaller matrix.  \n",
        "\n",
        "Below is an over-simplified version of `randomized_svd` (you wouldn't want to use this in practice, but it covers the core ideas).  The main part to notice is that we multiply our original matrix by a smaller random matrix (`M @ rand_matrix`) to produce `smaller_matrix`, and then use our same `np.linalg.svd` as before:\n",
        "\n",
        "```\n",
        "def randomized_svd(M, k=10):\n",
        "    m, n = M.shape\n",
        "    transpose = False\n",
        "    if m < n:\n",
        "        transpose = True\n",
        "        M = M.T\n",
        "        \n",
        "    rand_matrix = np.random.normal(size=(M.shape[1], k))  # short side by k\n",
        "    Q, _ = np.linalg.qr(M @ rand_matrix, mode='reduced')  # long side by k\n",
        "    smaller_matrix = Q.T @ M                              # k by short side\n",
        "    U_hat, s, V = np.linalg.svd(smaller_matrix, full_matrices=False)\n",
        "    U = Q @ U_hat\n",
        "    \n",
        "    if transpose:\n",
        "        return V.T, s.T, U.T\n",
        "    else:\n",
        "        return U, s, V\n",
        "```\n",
        "\n",
        "This code snippet is from this [randomized-SVD jupyter notebook](https://github.com/fastai/randomized-SVD/blob/master/Randomized%20SVD.ipynb) which was the demo I used for my PyBay talk on [Using randomness to make code much faster](https://www.youtube.com/watch?v=7i6kBz1kZ-A&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "445KmkaHgGR6",
        "colab_type": "text"
      },
      "source": [
        "**Dimension calculation**\n",
        "```\n",
        "M = m x n\n",
        "rand_matrix = n x k\n",
        "Q = M @ rand_matrix = m x k\n",
        "smaller_matrix = Q.T @ M = k x n\n",
        "U_hat = k x k\n",
        "U = Q @ U_hat = m x k\n",
        "```\n",
        "Therefore ultimately, k topics m documents containing svd decomposition done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "l9xzMfVacluS",
        "colab_type": "text"
      },
      "source": [
        "## Bayes Theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "lEmYAyYkcluU",
        "colab_type": "text"
      },
      "source": [
        "*Ex*: Physicist Leonard Mlodinow tested positive for HIV in 1989.  \n",
        "\tHis doctor said there was a 99.9% chance he had HIV.  \n",
        "   \n",
        "A = positive test results\t\t\n",
        "B = having HIV\n",
        "\n",
        "\n",
        "True positives: \t$P(A|B) = 99.9\\%$\t\n",
        "\n",
        "Prevalence:\t$P(B)= 0.01\\%$\n",
        "\n",
        "False positives:\t$P(A|B^C) = 0.1\\%$\t\n",
        "\n",
        "Was his doctor correct?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "HufHtDokcluV",
        "colab_type": "text"
      },
      "source": [
        "This example is from the book:\n",
        "\n",
        "<img src=\"https://github.com/fastai/course-nlp/blob/master/images/drunkards-walk.jpg?raw=1\" alt=\"drunkard's walk\" style=\"width: 30%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GFyac4orcluX",
        "colab_type": "text"
      },
      "source": [
        "Bayes Theorem (for conditional probabilities): $$ P(A | B) P(B) = P(B | A) P(A) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c_ED4CGHtmY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-HDbXAHwGP",
        "colab_type": "text"
      },
      "source": [
        "### My solution:\n",
        "\n",
        "![bayes solution](https://raw.githubusercontent.com/nlp-club/aisha_khatun_nlp_club/master/fast.ai%20NLP%20course/bayes.jpg)\n",
        "\n",
        "https://www4.stat.ncsu.edu/~reich/st590/code/HIV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "i6XXf8Y4cluY",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ni_3joEhcO3",
        "colab_type": "text"
      },
      "source": [
        "# Exercise \n",
        "\n",
        "<img src=\"https://github.com/fastai/course-nlp/blob/master/images/mlodinow-false-pos.png?raw=1\" alt=\"Mlodinow\" style=\"width: 80%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "kDn14uyYclui",
        "colab_type": "text"
      },
      "source": [
        "## Derivation of Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "eBQtNT7vcluj",
        "colab_type": "text"
      },
      "source": [
        "We want to calculate the probability that the review \"I loved it\" is positive.  Using Bayes Theorem, we can rewrite this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "m3ajvfFZclul",
        "colab_type": "text"
      },
      "source": [
        "$$ P(\\text{pos} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}, | \\text{pos}) \\cdot P(\\text{\"loved\"} | \\text{pos}) \\cdot P(\\text{\"it\"} | \\text{pos}) \\cdot P(\\text{pos})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$\n",
        "\n",
        "The \"naive\" part of Naive Bayes is that we will assume that the probabilities of the different words are all independent.\n",
        "\n",
        "$$ P(\\text{pos} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"} | \\text{pos}) \\cdot P(\\text{\"loved\"} | \\text{pos}) \\cdot P(\\text{\"it\"} | \\text{pos}) \\cdot P(\\text{pos})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9vD0Uk4vclum",
        "colab_type": "text"
      },
      "source": [
        "We do the same calculation to see how likely it is the review is negative, and then choose whichever is larger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MmnDJ-Jpcluo",
        "colab_type": "text"
      },
      "source": [
        "$$ P(\\text{neg} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"} | \\text{neg}) \\cdot P(\\text{\"loved\"} | \\text{neg}) \\cdot P(\\text{\"it\"} | \\text{neg}) \\cdot P(\\text{neg})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "N4CzbI2Sclup",
        "colab_type": "text"
      },
      "source": [
        "We will add one to avoid dividing by zero (or something close to it).  Similarly, we take logarithms to avoid multiplying by a lot of tiny values.  For the reasons we want to avoid this, please see the next section on numerical stability:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Nt9qvz99clur",
        "colab_type": "text"
      },
      "source": [
        "More reading: [Using log-probabilities for Naive Bayes](http://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "EiTTgHjBclus",
        "colab_type": "text"
      },
      "source": [
        "## Numerical Stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "IOkJwUObcluu",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ZS2gFq9Wcluv",
        "colab_type": "text"
      },
      "source": [
        "Take a moment to look at the function $f$ below.  Before you try running it, write on paper what the output would be of $x_1 = f(\\frac{1}{10})$.  Now, (still on paper) plug that back into $f$ and calculate $x_2 = f(x_1)$.  Keep going for 10 iterations.\n",
        "\n",
        "This example is taken from page 107 of *Numerical Methods*, by Greenbaum and Chartier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "qeDz1P2zclux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    if x <= 1/2:\n",
        "        return 2 * x\n",
        "    if x > 1/2:\n",
        "        return 2*x - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7v6NOCZKJAv",
        "colab_type": "text"
      },
      "source": [
        "1.   0.1\n",
        "2.   0.2\n",
        "3.   0.4\n",
        "5.   0.8\n",
        "6.   0.6\n",
        "7.   0.2\n",
        "8.   0.4\n",
        "9.   0.8\n",
        "10.  0.6\n",
        "11.  0.2\n",
        "12.  **0.4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "urKF9Ubcclu2",
        "colab_type": "text"
      },
      "source": [
        "Only after you've written down what you think the answer should be, run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "D0bZcS78clu3",
        "colab_type": "code",
        "outputId": "30d488ce-40cb-4fca-cfc4-b771d3523382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x = 1/10\n",
        "for i in range(80):\n",
        "    print(x)\n",
        "    x = f(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n",
            "0.2\n",
            "0.4\n",
            "0.8\n",
            "0.6000000000000001\n",
            "0.20000000000000018\n",
            "0.40000000000000036\n",
            "0.8000000000000007\n",
            "0.6000000000000014\n",
            "0.20000000000000284\n",
            "0.4000000000000057\n",
            "0.8000000000000114\n",
            "0.6000000000000227\n",
            "0.20000000000004547\n",
            "0.40000000000009095\n",
            "0.8000000000001819\n",
            "0.6000000000003638\n",
            "0.2000000000007276\n",
            "0.4000000000014552\n",
            "0.8000000000029104\n",
            "0.6000000000058208\n",
            "0.20000000001164153\n",
            "0.40000000002328306\n",
            "0.8000000000465661\n",
            "0.6000000000931323\n",
            "0.20000000018626451\n",
            "0.40000000037252903\n",
            "0.8000000007450581\n",
            "0.6000000014901161\n",
            "0.20000000298023224\n",
            "0.4000000059604645\n",
            "0.800000011920929\n",
            "0.6000000238418579\n",
            "0.20000004768371582\n",
            "0.40000009536743164\n",
            "0.8000001907348633\n",
            "0.6000003814697266\n",
            "0.20000076293945312\n",
            "0.40000152587890625\n",
            "0.8000030517578125\n",
            "0.600006103515625\n",
            "0.20001220703125\n",
            "0.4000244140625\n",
            "0.800048828125\n",
            "0.60009765625\n",
            "0.2001953125\n",
            "0.400390625\n",
            "0.80078125\n",
            "0.6015625\n",
            "0.203125\n",
            "0.40625\n",
            "0.8125\n",
            "0.625\n",
            "0.25\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "V4Ys2ZpPclu8",
        "colab_type": "text"
      },
      "source": [
        "What went wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "UNHgfzefclu-",
        "colab_type": "text"
      },
      "source": [
        "### Problem: math is continuous & infinite, but computers are discrete & finite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "9dqaaVvFclvA",
        "colab_type": "text"
      },
      "source": [
        "Two Limitations of computer representations of numbers:\n",
        "1. they can't be arbitrarily large or small\n",
        "2. there must be gaps between them\n",
        "\n",
        "The reason we need to care about accuracy, is because computers can't store infinitely accurate numbers.  It's possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since **each operation could multiply the error**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "BpY8XA7SclvB",
        "colab_type": "text"
      },
      "source": [
        "How computers store numbers:\n",
        "\n",
        "<img src=\"https://github.com/fastai/course-nlp/blob/master/images/fpa.png?raw=1\" alt=\"floating point\" style=\"width: 60%\"/>\n",
        "\n",
        "The *mantissa* can also be referred to as the *significand*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "O-_HAgr9clvD",
        "colab_type": "text"
      },
      "source": [
        "**IEEE Double precision arithmetic:**\n",
        "- Numbers can be as large as $1.79 \\times 10^{308}$ and as small as $2.23 \\times 10^{-308}$.\n",
        "- The interval $[1,2]$ is represented by discrete subset: \n",
        "$$1, \\: 1+2^{-52}, \\: 1+2 \\times 2^{-52},\\: 1+3 \\times 2^{-52},\\: \\ldots, 2$$\n",
        "\n",
        "- The interval $[2,4]$ is represented:\n",
        "$$2, \\: 2+2^{-51}, \\: 2+2 \\times 2^{-51},\\: 2+3 \\times 2^{-51},\\: \\ldots, 4$$\n",
        "\n",
        "- number of numbers bwtween 2 and 4 is the same as the number of numbers between 1 and 2. so with larger distances, the gaps also becomes larger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-9SBnLW-clvE",
        "colab_type": "text"
      },
      "source": [
        "Floats and doubles are not equidistant:\n",
        "\n",
        "<img src=\"https://github.com/fastai/course-nlp/blob/master/images/fltscale-wh.png?raw=1\" alt=\"floating point\" style=\"width: 100%\"/>\n",
        "Source: [What you never wanted to know about floating point but will be forced to find out](http://www.volkerschatz.com/science/float.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MwrhqDLCclvG",
        "colab_type": "text"
      },
      "source": [
        "**Machine Epsilon**\n",
        "\n",
        "Half the distance between 1 and the next larger number. This can vary by computer.  IEEE standards for double precision specify $$ \\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "VdL974CDclvH",
        "colab_type": "text"
      },
      "source": [
        "**Two important properties of Floating Point Arithmetic**:\n",
        "\n",
        "- The difference between a real number $x$ and its closest floating point approximation $fl(x)$ is always smaller than $\\varepsilon_{machine}$ in relative terms.  For some $\\varepsilon$, where $\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}$, $$fl(x)=x \\cdot (1 + \\varepsilon)$$\n",
        "\n",
        "- Where * is any operation ($+, -, \\times, \\div$), and $\\circledast$ is its floating point analogue,\n",
        "    $$ x \\circledast y = (x * y)(1 + \\varepsilon)$$\n",
        "for some $\\varepsilon$, where $\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}$\n",
        "That is, every operation of floating point arithmetic is exact up to a relative error of size at most $\\varepsilon_{machine}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "geL01PkPclvJ",
        "colab_type": "text"
      },
      "source": [
        "## Speed of different types of memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "sHCZMtVgclvK",
        "colab_type": "text"
      },
      "source": [
        "This course is 90% NLP and 10% things I want to make sure you see before the end of your MSDS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "0CD3m340clvM",
        "colab_type": "text"
      },
      "source": [
        "Here are some *numbers everyone should know* (from the legendary [Jeff Dean](http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/stanford-295-talk.pdf)):\n",
        "- L1 cache reference 0.5 ns\n",
        "- L2 cache reference 7 ns\n",
        "- Main memory reference/RAM 100 ns\n",
        "- Send 2K bytes over 1 Gbps network 20,000 ns\n",
        "- Read 1 MB sequentially from memory 250,000 ns\n",
        "- Round trip within same datacenter 500,000 ns\n",
        "- Disk seek 10,000,000 ns\n",
        "- Read 1 MB sequentially from network 10,000,000 ns\n",
        "- Read 1 MB sequentially from disk 30,000,000 ns\n",
        "- Send packet CA->Netherlands->CA 150,000,000 ns\n",
        "\n",
        "And here is an updated, interactive [version](https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html), which includes a timeline of how these numbers have changed.\n",
        "\n",
        "**Key take-away**: Each successive memory type is (at least) an order of magnitude worse than the one before it.  Disk seeks are **very slow**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Lq5ViIz2clvN",
        "colab_type": "text"
      },
      "source": [
        "## Revisiting Naive Bayes in an Excel Spreadsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "mZCLRUFAclvP",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate naive bayes in a spreadsheet to get a more visual picture of what is going on.  Here's how I processed the data for this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "WFE_nK5aclvQ",
        "colab_type": "text"
      },
      "source": [
        "### Loading our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "Z0yuOW4UclvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "1_r474FgclvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "KYLXX_8Cclvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_reviews = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
        "                         .split_from_df(col=2)\n",
        "                         .label_from_df(cols=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "zDkas0aHclvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_term_doc_matrix(label_list, vocab_len):\n",
        "    j_indices = []\n",
        "    indptr = []\n",
        "    values = []\n",
        "    indptr.append(0)\n",
        "\n",
        "    for i, doc in enumerate(label_list):\n",
        "        feature_counter = Counter(doc.data)\n",
        "        j_indices.extend(feature_counter.keys())\n",
        "        values.extend(feature_counter.values())\n",
        "        indptr.append(len(j_indices))\n",
        "        \n",
        "#     return (values, j_indices, indptr)\n",
        "\n",
        "    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
        "                                   shape=(len(indptr) - 1, vocab_len),\n",
        "                                   dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "6qaxmzNfclvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_term_doc = get_term_doc_matrix(movie_reviews.train.x, len(movie_reviews.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "zHcBD3Iiclvv",
        "colab_type": "text"
      },
      "source": [
        "### Getting data for our spreadsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "14jkI67Uclvw",
        "colab_type": "text"
      },
      "source": [
        "To keep our spreadsheet manageable, we will just get the 40 shortest reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "JWfIujLeclvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inds = np.argpartition(np.count_nonzero(trn_term_doc.todense(), 1), 40, axis=0)[:40]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqDsITA3cdrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba35d841-2676-4fd2-e39a-847fd29e806a"
      },
      "source": [
        "inds.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b0g1ufncaAK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "78213033-6ad8-4a0a-aea4-194553184fad"
      },
      "source": [
        "inds"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[597],\n",
              "        [170],\n",
              "        [320],\n",
              "        [119],\n",
              "        [730],\n",
              "        [386],\n",
              "        [428],\n",
              "        [245],\n",
              "        [467],\n",
              "        [102],\n",
              "        [477],\n",
              "        [210],\n",
              "        [ 53],\n",
              "        [212],\n",
              "        [ 42],\n",
              "        [336],\n",
              "        [395],\n",
              "        [275],\n",
              "        [533],\n",
              "        [276],\n",
              "        [499],\n",
              "        [630],\n",
              "        [264],\n",
              "        [697],\n",
              "        [461],\n",
              "        [441],\n",
              "        [122],\n",
              "        [704],\n",
              "        [790],\n",
              "        [712],\n",
              "        [756],\n",
              "        [254],\n",
              "        [369],\n",
              "        [483],\n",
              "        [617],\n",
              "        [607],\n",
              "        [699],\n",
              "        [646],\n",
              "        [ 46],\n",
              "        [328]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "_mKekkEEclv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inds = np.squeeze(np.asarray(inds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xDWyFdUcq3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c58d1437-7002-410c-e6e3-0a7338b503de"
      },
      "source": [
        "inds"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([597, 170, 320, 119, 730, 386, 428, 245, 467, 102, 477, 210,  53, 212,  42, 336, 395, 275, 533, 276, 499, 630,\n",
              "       264, 697, 461, 441, 122, 704, 790, 712, 756, 254, 369, 483, 617, 607, 699, 646,  46, 328])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Vx0lFcL9clwH",
        "colab_type": "text"
      },
      "source": [
        "Let's get the text from these 40 shortest reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "JKPvHcJOclwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_text = [movie_reviews.train.x[i].text for i in inds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-UQa6NUTclwT",
        "colab_type": "text"
      },
      "source": [
        "Get counts for all vocab used in our selection of the 40 shortest reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "6n00RjEOclwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_used = defaultdict(int)\n",
        "\n",
        "for i in inds:\n",
        "     for val in movie_reviews.train.x[i].data:\n",
        "        vocab_used[val] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBxaKbhOc9BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9549f795-aec8-40cc-f464-b40870f8ea73"
      },
      "source": [
        "len(vocab_used)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvVQyN2bc_hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "JtNnOZMFclwb",
        "colab_type": "text"
      },
      "source": [
        "Let's choose the words that are used at least 6 times (so not too rare), but less than 30 (so not too common).  You could try experimenting with different cut-off points on your own:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "XM-rzT1Cclwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interesting_inds = [key for key, val in vocab_used.items() if val < 30 and val >6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "FkE0jXS2clwk",
        "colab_type": "code",
        "outputId": "fff7d877-1c44-4988-b423-a5175f40f7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(interesting_inds)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9jqYHq_dKDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "927ba809-d508-4351-cfaa-48f5de16e0df"
      },
      "source": [
        "interesting_inds"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[53,\n",
              " 102,\n",
              " 71,\n",
              " 127,\n",
              " 24,\n",
              " 61,\n",
              " 6,\n",
              " 32,\n",
              " 50,\n",
              " 66,\n",
              " 59,\n",
              " 38,\n",
              " 28,\n",
              " 70,\n",
              " 44,\n",
              " 42,\n",
              " 46,\n",
              " 49,\n",
              " 75,\n",
              " 57,\n",
              " 51,\n",
              " 68,\n",
              " 113,\n",
              " 31,\n",
              " 99,\n",
              " 56,\n",
              " 120,\n",
              " 128,\n",
              " 30,\n",
              " 52,\n",
              " 88,\n",
              " 27,\n",
              " 23,\n",
              " 26,\n",
              " 20,\n",
              " 58,\n",
              " 22,\n",
              " 35,\n",
              " 37,\n",
              " 87,\n",
              " 25,\n",
              " 36,\n",
              " 34,\n",
              " 41]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3szIfouDclwp",
        "colab_type": "text"
      },
      "source": [
        "I copied the vocab and text of the movie reviews directly from here to paste into the spreadsheet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "B3BqTpi4clwr",
        "colab_type": "code",
        "outputId": "01e1a45f-3c32-48b2-8686-7de2cc45b3ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "[movie_reviews.vocab.itos[i] for i in interesting_inds]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['so',\n",
              " 'bad',\n",
              " 'very',\n",
              " 'acting',\n",
              " '\\n \\n ',\n",
              " 'do',\n",
              " 'xxup',\n",
              " 'film',\n",
              " 'from',\n",
              " 'some',\n",
              " 'if',\n",
              " 'are',\n",
              " 'for',\n",
              " 'up',\n",
              " 'one',\n",
              " 'have',\n",
              " 'all',\n",
              " 'an',\n",
              " 'no',\n",
              " 'just',\n",
              " 'like',\n",
              " 'good',\n",
              " 'great',\n",
              " 'but',\n",
              " '...',\n",
              " 'about',\n",
              " 'movies',\n",
              " 'seen',\n",
              " 'with',\n",
              " '!',\n",
              " 'me',\n",
              " 'as',\n",
              " \"'s\",\n",
              " 'was',\n",
              " 'that',\n",
              " 'out',\n",
              " '\"',\n",
              " 'on',\n",
              " \"n't\",\n",
              " 'story',\n",
              " '-',\n",
              " '(',\n",
              " ')',\n",
              " 'not']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "pKT2M6UGclwv",
        "colab_type": "code",
        "outputId": "63a4c94a-ccb5-4cb3-d43b-ad5265906021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "list_text"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"xxbos xxmaj this movie is so bad , i knew how it ends right after this little girl killed the first person . xxmaj very bad acting very bad plot very bad movie \\n \\n  do yourself a xxunk and xxup don't watch it 1 / 10\",\n",
              " 'xxbos i found this film to funny from the start . xxmaj john xxmaj waters use of characters reminded of some of the down to earth characters from xxmaj xxunk films . xxmaj christina xxmaj xxunk has once again xxunk her abilities in this film . xxmaj if you are looking for a fun movie without xxunk , i recommend this film .',\n",
              " 'xxbos xxmaj if you ever see a stand up comedy movie this is the one . xxmaj you will laugh xxunk if you have any sense of humor at all . xxmaj this is a once in a lifetime performance from a once in a lifetime performer . xxmaj this is a stand up standard .',\n",
              " 'xxbos xxmaj another movie to suffer without an adventure to run , no xxunk to solve . xxmaj just an xxunk man , acting like an animal . xxmaj no a good reason to take this journey . xxmaj pitt and xxmaj lewis are great actors ; magnificent xxmaj michelle xxmaj xxunk but a weak xxmaj david xxmaj duchovny performance ...',\n",
              " 'xxbos xxmaj this is an hilarious movie . xxmaj one of the very best things about it is the quality of the performance by each actor . xxmaj from the xxunk role to the xxunk , each character is vivid , xxunk and so understandable . xxmaj it can also make you laugh so hard your health will improve .',\n",
              " 'xxbos xxmaj this is by far one of the worst movies i have ever seen , the poor special effects along with the poor acting are just a few of the things wrong with this film . i am fan of the first two major xxunk but this one is lame !',\n",
              " \"xxbos xxmaj the direction struck me as poor man 's xxmaj xxunk xxmaj bergman . xxmaj the xxunk dialogue was annoying . xxmaj the xxunk xxunk that all characters except xxmaj xxunk ' showed made me think they were drugged . i think the director ruined it for me .\",\n",
              " 'xxbos xxmaj omar xxmaj xxunk is an outstanding actor . i really think he gets into his character xxunk . xxmaj when xxunk gets shot he shows true emotion . xxmaj he also shows true emotion when xxunk puts the gun to him in the room . xxmaj omar is a very talented actor ! !',\n",
              " 'xxbos xxmaj the greatest xxunk in life is being dull , and this movie is xxunk boring . its funny , its left out of his \" a life in film \" documentary . xxmaj he goes from a long piece on \" xxmaj xxunk xxmaj memories \" and then fast forwards to \" xxmaj xxunk \" . xxmaj this little piece of xxunk xxunk just is n\\'t worth the effort .',\n",
              " \"xxbos xxmaj the way the story is developed , keeps the audience wondering what is the xxunk 's dark past . xxmaj we get some clues during the series , but enough to keep us interested in the mini - series . xxmaj the characters are all believable and i personally felt xxunk and xxunk by the story .\",\n",
              " 'xxbos xxmaj this movie is one of the masterpieces from xxmaj mr. xxmaj xxunk . xxmaj it is about youth , xxunk , happiness , xxunk , xxunk , honor , corruption . xxmaj and it is like everything else from great xxmaj italian director xxunk art . \\n \\n ',\n",
              " \"xxbos xxmaj it 's terrific when a funny movie does n't make smile you . xxmaj what a pity ! ! xxmaj this film is very boring and so long . xxmaj it 's simply xxunk . xxmaj the story is staggering without goal and no fun . \\n \\n  xxmaj you feel better when it 's finished .\",\n",
              " 'xxbos \" xxmaj foxes \" is a great film . xxmaj the four young actresses xxmaj xxunk xxmaj foster , xxmaj xxunk xxmaj xxunk , xxmaj marilyn xxmaj xxunk and xxmaj xxunk xxmaj xxunk are wonderful . xxmaj the song \" xxmaj on the radio \" by xxmaj donna xxmaj summer is lovely . a great film . xxrep 5 *',\n",
              " 'xxbos xxmaj this movie features xxmaj charlie xxmaj xxunk dancing in a strip club . xxmaj beyond that , it features a truly bad script with dull , unrealistic dialogue . xxmaj that it got as many positive xxunk suggests some people may be joking .',\n",
              " 'xxbos xxmaj this film is brilliant ! xxmaj it touches everyone who sees it in an extraordinary way . xxmaj it really takes you back to your youth and puts a new perspective on how you view your childhood memories . xxmaj there are so many layers to this film . xxmaj it is innovative and absolutely fabulous !',\n",
              " \"xxbos i understand there was some conflict between xxmaj xxunk and the great xxmaj xxunk xxmaj smith during the filming . xxmaj understandable when you put one of the world 's greatest actresses of all time ( xxmaj smith , of course ) with one whose performances seem to get worse with each subsequent film .\",\n",
              " \"xxbos ... okay , maybe not all of it . xxmaj xxunk by the false promise of bikini - xxunk women on the movie 's cover ... but the xxup horror ... xxup the xxup horror ... ... whatever you do , do xxup not watch this movie . xxmaj xxunk out your eyes , repeatedly xxunk your xxunk in ... do what it takes . xxmaj never again -- never forget ! \\n \\n \",\n",
              " 'xxbos a bit of xxmaj trivia b / c i ca n\\'t figure out how to submit xxmaj trivia : xxmaj in the backdrop of this performance , one of the images is \\n \\n  xxmaj george xxmaj xxunk \\'s \" a xxmaj sunday xxmaj afternoon on the xxmaj island of xxmaj la xxmaj grande xxmaj xxunk \" painting ( seen best in chapter 18 ) , this painting is the subject of a xxmaj xxunk musical xxmaj sunday in the xxmaj park with xxmaj george . \\n \\n  a bit of xxmaj trivia b / c i ca n\\'t figure out how to submit xxmaj trivia : xxmaj in the backdrop of this performance , one of the images is \\n \\n  xxmaj george xxmaj xxunk \\'s \" a xxmaj sunday xxmaj afternoon on the xxmaj island of xxmaj la xxmaj grande xxmaj xxunk \" painting ( seen best in chapter 18 ) , this painting is the subject of a xxmaj xxunk musical xxmaj sunday in the xxmaj park with xxmaj george .',\n",
              " \"xxbos xxmaj it 's a good movie if you plan to watch lots of landscapes and animals , like an animal documentary . xxmaj and making xxmaj pierce xxmaj xxunk an indian make you wonder ' xxmaj does all those people do n't recognize if someone is n't indian at plain sight ? '\",\n",
              " \"xxbos xxmaj very nice action with an xxunk story which actually does n't suck . xxmaj interesting enough to merit watching instead of skipping past to get to the good parts . xxmaj having xxmaj jenna xxmaj xxunk and xxmaj asia xxmaj xxunk helps xxunk it up , too . xxmaj jenna in that xxunk and those xxunk is just xxunk ! xxmaj worth picking up just to see her !\",\n",
              " \"xxbos a very realistic portrait of a broken family and the effect it has on the kid caught in between . xxmaj as a child of divorced parents i was totally relating to events in the film . xxmaj also - a really cool zombie twist which i thought was xxup very xxup original . i 'm tired of the same old stuff in movies . a very realistic portrait of a broken family and the effect it has on the kid caught in between . xxmaj as a child of divorced parents i was totally relating to events in the film . xxmaj also - a really cool zombie twist which i thought was xxup very xxup original . i 'm tired of the same old stuff in movies . a very realistic portrait of a broken family and the effect it has on the kid caught in between . xxmaj as a child of divorced parents i was totally relating to events in the film . xxmaj also - a really cool zombie twist which i thought was xxup very xxup original . i 'm tired of the same old stuff in movies .\",\n",
              " \"xxbos xxmaj the wit and pace and three show stopping xxmaj xxunk xxmaj xxunk numbers put this ahead of the over - rated xxunk xxmaj street . xxmaj this is the xxunk 30 's musical with a knockout xxunk performance from xxmaj jimmy xxmaj cagney . xxmaj one of the last xxunk before the xxmaj motion xxmaj picture xxmaj production xxmaj code was strictly xxunk . a must see .\",\n",
              " 'xxbos xxmaj this film is scary because you can find yourself relating to ideas they have and can recall other people saying and having xxunk ideas make this a haunting well done movie xxrep 4 . the xxunk style is not xxunk to point it xxunk you out of film like blair witch it only adds to the raw \" real \" feeling of the film that makes it .',\n",
              " \"xxbos i did enjoy this film , i thought it ended up being an old fashioned love story with a few twists . i expected him to get the girl , i wo n't tell you if he does or not you will need to watch the movie to find out . xxmaj overall if you are looking to watch a love story this one will suffice .\",\n",
              " 'xxbos xxmaj this movie was so bad that my xxunk . went down about 40 points after seeing it . xxmaj it made me wonder who could sit through the weeks it took to make it and think that it was worth it . xxmaj it must of been some kind of personal favor to xxmaj van xxmaj damme .',\n",
              " 'xxbos i can not believe how unknown this movie is , it was absolutely incredible . xxmaj the ending alone has stuck with me for almost thirty years . xxmaj the road sign through the xxunk mirror blew me away . xxmaj if you liked \" xxup race xxup with xxup the xxup devil \" you will love this movie',\n",
              " \"xxbos xxmaj this was a horrible film ! i gave it 2 xxmaj points , one for xxmaj xxunk xxmaj jolie and a second one for the beautiful xxmaj xxunk in the beginning ... xxmaj other than that the story just plain sucked and cars xxunk through cities was n't so new in 1970 . xxmaj the xxmaj xxunk was probably what annoyed me the most , xxunk seen anything so constructed !\",\n",
              " \"xxbos xxmaj good actors and good performances ca n't mask a pointless script , bad dialogue , and patterns of behavior xxunk into nothing you 'd care about . xxmaj the most interesting character is xxmaj david xxmaj xxunk . xxmaj no character development - no xxunk , no interest , just some suffering for no particular reason , teaching us nothing and not even xxunk to entertain .\",\n",
              " 'xxbos xxmaj yes , i \\'m sentimental & xxunk ! ! xxmaj but this movie ( and it \\'s theme song ) remain one of my all time xxunk ! ! xxmaj robert xxmaj xxunk xxmaj jr. does such justice to the role of \" xxmaj louis xxmaj xxunk \" xxunk and the storyline ( although far - xxunk ) is romantic & makes one believe in happy xxunk ! !',\n",
              " \"xxbos i thoroughly enjoyed this film for its humor and xxunk . i especially like the way the characters welcomed xxmaj gina 's various suitors . xxmaj with friends ( and family ) like these anyone would feel xxunk and loved . i found the writing witty and natural and the actors made the material come alive .\",\n",
              " \"xxbos i love this movie . i mean the story may not be the best , but the dancing most certainly makes up for it . xxmaj you get to know a little bit about each character the way xxup they want you to learn about them . i just think that you wo n't like this movie unless you 're into xxmaj broadway ...\",\n",
              " \"xxbos i and a friend rented this movie . xxmaj we both found the movie soundtrack and production techniques to be xxunk . xxmaj the movie 's plot appeared to drag on throughout with little surprise in the ending . xxmaj we both xxunk that the movie could have been xxunk into roughly an hour giving it more suspense and moving plot .\",\n",
              " 'xxbos xxmaj this movie was amazingly bad . i do n\\'t think i \\'ve ever seen a movie where every attempt at humor failed as miserably . xxmaj let \\'s see ... the acting was pathetic , the \" special effects \" where horrible , the plot non - xxunk ... that pretty much xxunk up this movie . \\n \\n ',\n",
              " 'xxbos xxmaj this is a worthless sequel to a great action movie . xxmaj cheap looking , and worst of all , xxup boring xxup action xxup scenes ! xxmaj the only decent thing about the movie is the last fight sequence . xxmaj only xxunk minutes , but it feels like it goes on forever ! xxmaj even die - hard xxmaj van xxmaj damme xxunk myself ) should avoid this one !',\n",
              " 'xxbos xxmaj beautiful and touching movie . xxmaj rich colors , great settings , good acting and one of the most charming movies i have seen in a while . i never saw such an interesting setting when i was in xxmaj china . xxmaj my wife liked it so much she asked me to xxunk on and rate it so other would enjoy too .',\n",
              " \"xxbos xxmaj this film is the worst film i have ever seen . xxmaj the story line is weak - i could n't even follow it . xxmaj the acting is high - xxunk . xxmaj the sound track is irritating . xxmaj the attempts at humor are not . xxmaj the editing is horrible . xxmaj the credits are even slow - i would be embarrassed to have my name associated with this waste of film . xxmaj do n't waste your time even thinking about this attempt at acting .\",\n",
              " 'xxbos i enjoyed this movie . xxmaj unlike like some of the xxunk up , xxunk trash that is passed off as action movies , xxmaj playing xxmaj god is simple and realistic , with characters that are believable , action that is not over the top and enough twists and turns to keep you interested until the end . \\n \\n  xxmaj well directed , well acted and a good story .',\n",
              " 'xxbos i caught this movie about 8 years ago , and have never had it of my mind . surely someone out there will release it on xxmaj video , or hey why not xxup dvd ! xxmaj the ford xxunk is the star xxrep 7 . if you have any head for cars xxup watch xxup this and be blown away .',\n",
              " \"xxbos all i have to say is if you do n't like it then there is something wrong with you . plus xxmaj jessica is just all kinds of hot xxrep 5 ! the only reason you may not like it is because it is set in the future where xxmaj xxunk has gone to hell . that and you my not like it cause the future they show could very well happen .\",\n",
              " \"xxbos xxmaj this tale set in xxmaj xxunk , xxmaj new xxmaj zealand xxunk ( xxmaj xxunk xxunk of the xxunk xxmaj xxunk xxmaj college ) is xxunk 's first feature . \\n \\n  xxmaj with a contemporary xxmaj new xxmaj zealand xxunk xxmaj via xxmaj xxunk xxunk with absolutely hilarious situations which develop in the ( adult ) family context . xxmaj at the same time it manages to xxunk intense emotions of sadness and despair . \\n \\n  xxmaj one of the most moving and xxunk movies of the year - not to be missed !\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "XD6-EAi5clwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc[inds,:]\n",
        "y = movie_reviews.train.y[inds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "HiZp1-vnclw5",
        "colab_type": "text"
      },
      "source": [
        "#### Export to CSVs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "XUsEVvL4clw6",
        "colab_type": "text"
      },
      "source": [
        "Let's export the term-document matrix and the labels to CSVs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "L3GXpVE3clw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import FileLink, FileLinks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "qasN32feclxC",
        "colab_type": "code",
        "outputId": "fe34fae8-b22c-474a-ec01-364f26befa04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.savetxt(\"x.csv\", x.todense()[:,interesting_inds], delimiter=\",\", fmt='%.14f')\n",
        "FileLink('x.csv')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<a href='x.csv' target='_blank'>x.csv</a><br>"
            ],
            "text/plain": [
              "/content/x.csv"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "fCk8lmlcclxF",
        "colab_type": "code",
        "outputId": "4324b299-035d-4ccb-ef12-4b6e27868e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.savetxt(\"y.csv\", y, delimiter=\",\", fmt=\"%i\")\n",
        "FileLink('y.csv')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<a href='y.csv' target='_blank'>y.csv</a><br>"
            ],
            "text/plain": [
              "/content/y.csv"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWIaWMnSdpHY",
        "colab_type": "text"
      },
      "source": [
        "## Check Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ByTXT4fd5qC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ebc64d4-9cb7-41d9-bdf5-b5b1cd424b09"
      },
      "source": [
        "interesting_inds[:10]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[53, 102, 71, 127, 24, 61, 6, 32, 50, 66]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e90TFjEHdkoR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6baa13ae-0c15-4003-c818-219a59aa6ecc"
      },
      "source": [
        "x = pd.read_csv('x.csv',header=None)\n",
        "x.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0    1    2    3    4    5    6   ...   37   38   39   40   41   42   43\n",
              "0  1.0  4.0  3.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4  2.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTYywip3eIfl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39368797-277b-4165-9292-95ea9e8a9eb3"
      },
      "source": [
        "len(x)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LAepRdceLVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9705e9f6-e1ed-4588-f76b-85c3092ef929"
      },
      "source": [
        "x.columns"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Dz7_KAeOWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69eabbdb-035c-40f2-e8ab-754f8d1fba1e"
      },
      "source": [
        "len(x.columns) ## number of vocabs"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TgF647sdvBy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9bb6f1eb-23b4-484d-82ae-6fcf84ae6bb6"
      },
      "source": [
        "y = pd.read_csv('y.csv',header=None)\n",
        "y.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  0\n",
              "1  1\n",
              "2  1\n",
              "3  0\n",
              "4  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DySsS5qfeJ9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb1e311f-ada8-4653-ebfb-75d74a5472f6"
      },
      "source": [
        "len(y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzyEV9_8dxWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}